{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "#add path\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_dir, '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocess_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "preprocess_data(\"../telegram_messages.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.CoNLL_formatting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         channel         sender                  timestamp  \\\n",
      "0  @ZemenExpress -1001307493052  2025-01-15 05:43:58+00:00   \n",
      "1  @ZemenExpress -1001307493052  2025-01-15 05:43:58+00:00   \n",
      "2  @ZemenExpress -1001307493052  2025-01-14 13:29:49+00:00   \n",
      "3  @ZemenExpress -1001307493052  2025-01-14 13:29:49+00:00   \n",
      "4  @ZemenExpress -1001307493052  2025-01-14 08:17:05+00:00   \n",
      "\n",
      "                                             content  \\\n",
      "0                                                NaN   \n",
      "1  💥💥..........................💥💥\\n              ...   \n",
      "2                                                NaN   \n",
      "3  💥💥...................................💥💥\\n\\n📌Fe...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                              tokens  \n",
      "0                                                 []  \n",
      "1  ['💥💥..........................💥💥', 'መልካም', 'በአ...  \n",
      "2                                                 []  \n",
      "3  ['💥💥...................................💥💥', '📌...  \n",
      "4                                                 []  \n",
      "Index(['channel', 'sender', 'timestamp', 'content', 'tokens'], dtype='object')\n",
      "Labeled data saved to labeled_data.conll\n"
     ]
    }
   ],
   "source": [
    "# Read the data and save the subset_df in a variable\n",
    "data = 'preprocessed_data.csv'\n",
    "subset_df = read_subset_df(data)\n",
    "\n",
    "# Call the label_data function to process and save the labeled data\n",
    "label_data(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.load_conll_dataset import *\n",
    "from scripts.tokenization_alignment import *\n",
    "from scripts.training_and_tuning import *\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables for api\n",
    "token = os.getenv('TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the connl data that we made earlier\n",
    "file_path = 'labeled_data.conll'\n",
    "sentences, labels = load_conll_dataset(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_codes()\n",
    "# sentences, labels = load_conll_dataset('labeled_data.conll')\n",
    "\n",
    "# Define label mapping before loading the model\n",
    "label_set = set(label for label_list in labels for label in label_list)\n",
    "label_map = {label: i for i, label in enumerate(label_set)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at rasyosef/bert-tiny-amharic and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"rasyosef/bert-tiny-amharic\"  # Change to \"bert-tiny-amharic\" or \"afroxmlr\" as needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m tokenized_dataset_full \u001b[38;5;241m=\u001b[39m tokenize_and_align_labels(sentences, labels, tokenizer, label_map)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and validation sets\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_sentences, val_sentences, train_labels, val_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Tokenize and align labels for train and validation sets\u001b[39;00m\n\u001b[0;32m      8\u001b[0m train_tokenized \u001b[38;5;241m=\u001b[39m tokenize_and_align_labels(train_sentences, train_labels, tokenizer, label_map)\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\10_Academy\\Week-5\\kaim-week-5\\.myvenv5\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\10_Academy\\Week-5\\kaim-week-5\\.myvenv5\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2851\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2848\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2850\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2851\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\10_Academy\\Week-5\\kaim-week-5\\.myvenv5\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2481\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2478\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2485\u001b[0m     )\n\u001b[0;32m   2487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize and align labels for the entire dataset\n",
    "tokenized_dataset_full = tokenize_and_align_labels(sentences, labels, tokenizer, label_map)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(sentences, labels, test_size=0.1)\n",
    "\n",
    "# Tokenize and align labels for train and validation sets\n",
    "train_tokenized = tokenize_and_align_labels(train_sentences, train_labels, tokenizer, label_map)\n",
    "val_tokenized = tokenize_and_align_labels(val_sentences, val_labels, tokenizer, label_map)\n",
    "\n",
    "# Create a dictionary for the tokenized dataset\n",
    "tokenized_dataset = {\n",
    "    'train': train_tokenized,\n",
    "    'validation': val_tokenized\n",
    "}\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer = fine_tune_model(tokenized_dataset['train'], tokenized_dataset['validation'], model, tokenizer)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = evaluate_model(trainer)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 3], 'token_type_ids': [0, 0], 'attention_mask': [1, 1], 'labels': []}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and align labels\n",
    "tokenized_dataset = tokenize_and_align_labels(sentences, labels, tokenizer, label_map)\n",
    "print(tokenized_dataset)\n",
    "\n",
    "# # Fine-tune the model\n",
    "# trainer = fine_tune_model(tokenized_dataset['train'], tokenized_dataset['validation'], model, tokenizer)\n",
    "\n",
    "# # Evaluate the model\n",
    "# eval_results = evaluate_model(trainer)\n",
    "# print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "save_model(trainer, tokenizer, \"fine_tuned_ner_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
